"""
This file provides a class that can learn APATE graph-based features. Implementation based on:

"APATE: A novel approach for automated credit card transaction fraud detection using network-
based extensions" by Véronique Van Vlasselaer, Cristián Bravo, Olivier Caelen, Tina
Eliassa-Rad, Leman Akoglu, Monique Snoeck, and Bart Baesens

"A graph-based, semi-supervised, credit card fraud detection system" by Bertand Lebichot, Fabian Braun,
Olivier Caelen, and Marco Saerens


Notation in the code (variable names) primarily based on notation used in the second paper of the two above.




The intuition of this feature engineering procedure is to construct a graph (using training data) where Merchants,
Card Holders and Transactions are nodes. This is a tripartite graph, where there are only edges between Merchants and
Transactions, and edges between Card Holders and Transactions (so no edges between two nodes of the same type, and no
edges directly between Card Holders and Merchants). Every transaction has exactly two edges; one with the Merchant,
and one with the Card Holder corresponding to that transaction.

Through a convergence procedure, ''influence'' of fraudulent transactions is spread throughout the network. This
influence will be higher in parts of the network where there is a high concentration of Merchants and/or Card
Holders that are frequently involved in fraudulent transactions.

After running the convergence procedure using training data, an object of this class can be used to provide various
scores (new features) for new transactions. For new transactions where the Card Holder as well as the Merchant
have never been observed in training data, these scores will always simply be 0 because we have no information on them.
For new transactions where the Card Holder and/or the Merchant have already previously been seen in the training data,
this procedure can provide useful scores related to how ''risky'' they are.

@author: Dennis Soemers
"""

import math
from scipy.sparse import coo_matrix
from scipy.sparse import diags


class ApateGraphFeatures:

    def __init__(self, training_data, convergence_threshold=0.0000001, alpha=0.85):
        """
        Constructs an ''APATE Graph Features'' object based on a given set of training data.
        This will run the convergence procedure on the entire training data, which can easily
        take several minutes for large datasets.

        Once this construction has finished, the object can be used at later points in time to
        generate new features for new transactions outside of the original training data.

        In the real-world setting described in the two papers this implementation is based on,
        this convergence procedure would be re-run on the latest available training data every day
        at midnight. So, for experiments simulating long-term scenarios, it is recommended to reconstruct
        new objects of this class, with new training data, at regular intervals.

        :param training_data:
            The dataset to use for generating the network and running the convergence procedure.
            Expected to be in the format generated by the data/preprocess_data_raw.py script.

        :param convergence_threshold:
            Threshold used to determine whether or not the scores have converged in the convergence
            procedure. When an iteration does not change any score by an amount that exceeds this value,
            the convergence procedure will be terminated.
        :param alpha:
            This parameter can be interpreted as the probability to continue a walk (instead of restarting)
            in a Random Walk with Restart Procedure. Both of the papers this implementation is based only
            mention using a value of 0.85
        """

        self.CONVERGENCE_THRESHOLD = convergence_threshold
        self.ALPHA = alpha

        self.cards_dict = {}
        self.merchants_dict = {}

        self.next_card_idx = 0
        self.next_merchant_idx = 0

        cards = list(training_data["CardID"])
        merchants = list(training_data["MerchantID"])

        self.num_t = len(cards)
        self.num_c = len(training_data["CardID"].unique())
        self.num_m = len(training_data["MerchantID"].unique())

        # compute A^{tri} adjacency matrices of tripartite graph with various levels of decay
        rows_A = []
        cols_A = []
        entries_A = []
        entries_A_short = []
        entries_A_medium = []
        entries_A_long = []

        # at the same time, we'll also compute the starting vectors r_0 with various levels of decay
        rows_r0 = []
        cols_r0 = []
        entries_r0 = []
        entries_r0_short = []
        entries_r0_medium = []
        entries_r0_long = []

        '''
        We'll use the latest date in the training data as ''test date''. Influence of older transactions
        will be decayed with respect to this date. I suppose that, ideally, we'd want to always decay with
        respect to a single test instance that we're generating features for. That would be completely infeasible
        computationally though, since it would mean re-running the entire convergence procedure over and over
        again for every single test instance.

        Another reasonable alternative could be to use the first date of a test set. It is not clear which timepoint
        was used in the two papers this implementation is based on.
        '''
        test_date = training_data.Global_Date.max()

        t = 0
        for row in training_data.itertuples():
            c = self.get_card_idx(cards[t])
            m = self.get_merchant_idx(merchants[t])

            # compute entries in adjacency matrix with different implementations of decay
            short_term_entry = self.compute_A_entry(row.Global_Date, test_date, gamma=0.03, interval="short")
            medium_term_entry = self.compute_A_entry(row.Global_Date, test_date, gamma=0.004, interval="medium")
            long_term_entry = self.compute_A_entry(row.Global_Date, test_date, gamma=0.0001, interval="long")

            # entry in A_{t * c}
            rows_A.append(t)
            cols_A.append(c + self.num_t)
            entries_A.append(1)
            entries_A_short.append(short_term_entry)
            entries_A_medium.append(medium_term_entry)
            entries_A_long.append(long_term_entry)

            # entry in A_{t * m}
            rows_A.append(t)
            cols_A.append(m + self.num_c + self.num_t)
            entries_A.append(1)
            entries_A_short.append(short_term_entry)
            entries_A_medium.append(medium_term_entry)
            entries_A_long.append(long_term_entry)

            # entry in A_{c * t}
            rows_A.append(c + self.num_t)
            cols_A.append(t)
            entries_A.append(1)
            entries_A_short.append(short_term_entry)
            entries_A_medium.append(medium_term_entry)
            entries_A_long.append(long_term_entry)

            # entry in A_{m * t}
            rows_A.append(m + self.num_c + self.num_t)
            cols_A.append(t)
            entries_A.append(1)
            entries_A_short.append(short_term_entry)
            entries_A_medium.append(medium_term_entry)
            entries_A_long.append(long_term_entry)

            if row.Target == 1:
                # fraudulent transaction, add entries to r_0 vectors
                rows_r0.append(t)
                cols_r0.append(0)
                entries_r0.append(1)
                entries_r0_short.append(short_term_entry)
                entries_r0_medium.append(medium_term_entry)
                entries_r0_long.append(long_term_entry)

            # increment transaction counter
            t += 1

        # we'll use a sparse matrix. May not yet be necessary for original dataset, but will be necessary
        # for larger datasets from simulator
        dim = self.num_t + self.num_c + self.num_m
        A_tri_shape = (dim, dim)

        self.A_tri = coo_matrix((entries_A, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_short = coo_matrix((entries_A_short, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_medium = coo_matrix((entries_A_medium, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_long = coo_matrix((entries_A_long, (rows_A, cols_A)), shape=A_tri_shape).tocsr()

        self.r0 = coo_matrix((entries_r0, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_short = coo_matrix((entries_r0_short, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_medium = coo_matrix((entries_r0_medium, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_long = coo_matrix((entries_r0_long, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()

        # normalize rows to sum up to 1.0 for all the A_tri matrices (first paper says normalize columns,
        # but we structured the matrix as in second paper, so we need to normalize rows)
        self.A_tri = self.normalize_rows(self.A_tri)
        self.A_tri_short = self.normalize_rows(self.A_tri_short)
        self.A_tri_medium = self.normalize_rows(self.A_tri_medium)
        self.A_tri_long = self.normalize_rows(self.A_tri_long)

        # normalize r0 vectors to sum up to 1.0 (this is described in the first article, but not the second)
        self.r0 = self.r0 / self.r0.sum()
        self.r0_short = self.r0_short / self.r0_short.sum()
        self.r0_medium = self.r0_medium / self.r0_medium.sum()
        self.r0_long = self.r0_long / self.r0_long.sum()

        # run convergence procedure for the four different levels of decay
        self.rkc = self.converge(self.r0, self.A_tri,
                                 alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        self.rkc_short = self.converge(self.r0_short, self.A_tri_short,
                                       alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        self.rkc_medium = self.converge(self.r0_medium, self.A_tri_medium,
                                        alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        self.rkc_long = self.converge(self.r0_long, self.A_tri_long,
                                      alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)

        # TODO may be a useful optimization to call eliminate_zeros() on all these csr matrices / csc vectors?

    def add_graph_features(self, test_data):
        """
        Adds 12 new features, based on the constructed network, to the entire given test dataset. The features
        added are:

        - Four risk scores for the Card Holder, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.
        - Four risk scores for the Merchant, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.
        - Four risk scores for the Transaction, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.

        :param test_data:
            A pandas dataframe containing all the transactions we want to add new features to.
        """

        # TODO I suspect a function which computes the 12 features just for a single row, instead of an entire
        # TODO dataframe, could be useful as well. This also depends a bit on the API of the simulator itself.

        # add four scores for card holder (four levels of decay)
        test_data["CHScore"] = test_data.apply(lambda row: self.get_ch_score(row, ""), axis=1)
        test_data["CHScore_ST"] = test_data.apply(lambda row: self.get_ch_score(row, "short"), axis=1)
        test_data["CHScore_MT"] = test_data.apply(lambda row: self.get_ch_score(row, "medium"), axis=1)
        test_data["CHScore_LT"] = test_data.apply(lambda row: self.get_ch_score(row, "long"), axis=1)

        # add four scores for merchant (four levels of decay)
        test_data["MerScore"] = test_data.apply(lambda row: self.get_mer_score(row, ""), axis=1)
        test_data["MerScore_ST"] = test_data.apply(lambda row: self.get_mer_score(row, "short"), axis=1)
        test_data["MerScore_MT"] = test_data.apply(lambda row: self.get_mer_score(row, "medium"), axis=1)
        test_data["MerScore_LT"] = test_data.apply(lambda row: self.get_mer_score(row, "long"), axis=1)

        # make sure all our A^{tri} matrices have sorted indices (important before computing transaction scores)
        if not self.A_tri.has_sorted_indices:
            self.A_tri.sort_indices()
        if not self.A_tri_short.has_sorted_indices:
            self.A_tri_short.sort_indices()
        if not self.A_tri_medium.has_sorted_indices:
            self.A_tri_medium.sort_indices()
        if not self.A_tri_long.has_sorted_indices:
            self.A_tri_long.sort_indices()

        # add four scores for transaction (four levels of decay)
        test_data["TrxScore"] = test_data.apply(lambda row: self.compute_trx_score(row, ""), axis=1)
        test_data["TrxScore_ST"] = test_data.apply(lambda row: self.compute_trx_score(row, "short"), axis=1)
        test_data["TrxScore_MT"] = test_data.apply(lambda row: self.compute_trx_score(row, "medium"), axis=1)
        test_data["TrxScore_LT"] = test_data.apply(lambda row: self.compute_trx_score(row, "long"), axis=1)

    def compute_trx_score(self, row, interval):
        """
        Computes transaction score for a given row from test data. Assumes card holder and merchant scores
        have already been computed and added

        :param row:
        :param interval:
        :return:
        """

        card = self.get_card_idx(row.CardID)
        merchant = self.get_merchant_idx(row.MerchantID)

        if card < self.num_c and merchant < self.num_m:
            # card ID and merchant ID both appear in training data, so it's possible we'll find a transaction
            # between them in training data

            # get the row in undecayed adjacency matrix with all previous transactions of this card ID
            card_t_row = self.A_tri.getrow(self.num_t + card)
            nonzero_rows, nonzero_cols = card_t_row.tocoo().nonzero()

            # if there are multiple transactions between same merchant and same card holder, we want score
            # of the latest transaction. So, we loop through transactions in reverse order
            for nonzero_col in reversed(nonzero_cols):
                # find the merchant corresponding to this transaction (with already matching card ID
                transaction_col = self.A_tri.getcol(nonzero_col)
                nonzero_rows_2, nonzero_cols2 = transaction_col.tocoo().nonzero()

                # there must be exactly two entries in nonzero_rows2: first for card ID (= card_t_row),
                # second for merchant (which is the row we want)
                if len(nonzero_rows_2) != 2:
                    print("ERROR: len(nonzero_rows_2) != 2")

                old_merchant_idx = nonzero_rows_2[1] - self.num_t - self.num_c

                if old_merchant_idx == merchant:
                    # found a match, use transaction scores of this transaction
                    if interval == "":
                        return self.rkc[nonzero_col, 0]
                    elif interval == "short":
                        return self.rkc_short[nonzero_col, 0]
                    elif interval == "medium":
                        return self.rkc_medium[nonzero_col, 0]
                    elif interval == "long":
                        return self.rkc_long[nonzero_col, 0]
                    else:
                        print("ERROR: unknown interval type!")

        # couldn't find transaction between same merchant and card holder in training data
        # so, use the ''local update'' rule to compute score for transaction
        merchant_influence = 0
        card_influence = 0

        if card < self.num_c:
            card_col = self.num_t + card

            if interval == "":
                card_influence = (1.0 / (self.A_tri.getcol(card_col).sum() + 1)) * row["CHScore"]
            elif interval == "short":
                card_influence = (1.0 / (self.A_tri_short.getcol(card_col).sum() + 1)) * row["CHScore_ST"]
            elif interval == "medium":
                card_influence = (1.0 / (self.A_tri_medium.getcol(card_col).sum() + 1)) * row["CHScore_MT"]
            elif interval == "long":
                card_influence = (1.0 / (self.A_tri_long.getcol(card_col).sum() + 1)) * row["CHScore_LT"]
            else:
                print("ERROR: unknown interval type!")

        if merchant < self.num_m:
            merchant_col = self.num_t + self.num_c + merchant

            if interval == "":
                merchant_influence = (1.0 / (self.A_tri.getcol(merchant_col).sum() + 1)) * row["MerScore"]
            elif interval == "short":
                merchant_influence = (1.0 / (self.A_tri_short.getcol(merchant_col).sum() + 1)) * row["MerScore_ST"]
            elif interval == "medium":
                merchant_influence = (1.0 / (self.A_tri_medium.getcol(merchant_col).sum() + 1)) * row["MerScore_MT"]
            elif interval == "long":
                merchant_influence = (1.0 / (self.A_tri_long.getcol(merchant_col).sum() + 1)) * row["MerScore_LT"]
            else:
                print("ERROR: unknown interval type!")

        # TODO: in cases where both merchant and card holder are unknown, this will always return 0.
        # TODO: It may be interesting to try using the average scores throughout the entire network instead?

        return merchant_influence + card_influence

    def compute_A_entry(self, transaction_date, test_date, gamma, interval):
        """
        Computes a correctly decayed entry for an Adjacency Matrix

        :param transaction_date:
            The date of the transaction corresponding to the matrix entry
        :param test_date:
            We'll decay with respect to this date
        :param gamma:
            Decay parameter
        :param interval:
            "short", "medium", or "long"
        :return:
            The value for the matrix entry
        """
        timedelta = test_date - transaction_date

        t = 0
        if interval == "short":
            t = timedelta.total_seconds() / 60          # use minutes
        elif interval == "medium":
            t = timedelta.total_seconds() / (60 * 60)   # use hours
        elif interval == "long":
            t = timedelta.days                  # use days
        else:
            print("WARNING: Unknown interval in compute_A_entry(): ", interval)

        # just a sanity check, we should have ''reverse decay'' if the given test_date is earlier
        # than some transactions in the training data
        if t < 0:
            t = 0

        # the decay as described in papers can easily lead to some extremely low values, causing numerical
        # instability later on. So we'll prevent it from decaying too far with the max()
        return max(math.exp(-gamma * t), 0.00000001)

    def converge(self, r0, A_tri, alpha, convergence_threshold):
        """
        Runs the convergence procedure to compute the final vector of risk scores

        :param r0:
            Starting vector of scores, which should simply contain decayed entries for all fraudulent transactions
            (and be normalized to sum up to 1 according to first paper, but not explicitly mentioned in second paper)
        :param A_tri:
            Adjacency matrix
        :param alpha:
            Alpha parameter
        :param convergence_threshold:
            Threshold to determine whether or not we reached convergence
        :return:
            The final, converged vector of risk scores
        """

        # following second paper (not first paper), we need to transpose A_tri (at this point in time, actually
        # called P in second paper)
        A_tri = A_tri.copy().transpose()
        r = r0.copy()

        while True:
            r_old = r
            r_new = alpha * (A_tri * r) + (1.0 - alpha) * r0

            max__abs_change = max((r_new - r_old).max(), (r_old - r_new).max())

            r = r_new

            if max__abs_change < convergence_threshold:
                # converged
                break

        return r

    def get_ch_score(self, row, interval):
        """
        Returns the score for the card holder

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for Card Holder
        """

        card = self.get_card_idx(row.CardID)

        if card >= self.num_c:
            # this card ID doesn't appear in training data at all
            return 0
        elif interval == "":
            return self.rkc[self.num_t + card, 0]
        elif interval == "short":
            return self.rkc_short[self.num_t + card, 0]
        elif interval == "medium":
            return self.rkc_medium[self.num_t + card, 0]
        elif interval == "long":
            return self.rkc_long[self.num_t + card, 0]

    def get_mer_score(self, row, interval):
        """
        Returns the score for the merchant

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for Merchant
        """

        merchant = self.get_merchant_idx(row.MerchantID)

        if merchant >= self.num_m:
            # this merchant ID doesn't appear in training data at all
            return 0
        elif interval == "":
            return self.rkc[self.num_t + self.num_c + merchant, 0]
        elif interval == "short":
            return self.rkc_short[self.num_t + self.num_c + merchant, 0]
        elif interval == "medium":
            return self.rkc_medium[self.num_t + self.num_c + merchant, 0]
        elif interval == "long":
            return self.rkc_long[self.num_t + self.num_c + merchant, 0]

    def normalize_rows(self, matrix):
        """
        Normalizes the rows of the given matrix, such that every row sums up to 1

        :param matrix:
            Matrix of which the rows should be normalized
        :return:
            Row-normalized version of the given matrix
        """

        # We'll normalize the row to sum up to 1 by pre-multiplying by a matrix
        # that contains the scalars for every row on the diagonal
        scalars = []

        for i in range(matrix.shape[0]):
            row = matrix.getrow(i)
            row_sum = row.sum()
            scalars.append(1.0 / row_sum)

        return diags(scalars, 0) * matrix

    def get_card_idx(self, card_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Card ID

        :param card_id:
            Card ID
        :return:
            Unique index (will be 0 for first Card ID observed, 1 for the next, etc.)
        """
        if card_id not in self.cards_dict:
            self.cards_dict[card_id] = self.next_card_idx
            self.next_card_idx += 1

        return self.cards_dict[card_id]

    def get_merchant_idx(self, merchant_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Merchant ID

        :param merchant_id:
            Merchant ID
        :return:
            Unique index (will be 0 for first Merchant ID observed, 1 for the next, etc.)
        """
        if merchant_id not in self.merchants_dict:
            self.merchants_dict[merchant_id] = self.next_merchant_idx
            self.next_merchant_idx += 1

        return self.merchants_dict[merchant_id]
